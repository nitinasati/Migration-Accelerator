#!/usr/bin/env python3
"""
Test the complete workflow JSON response with MCP tools simulation.
"""

import asyncio
import os
from config.settings import LLMConfig, LLMProvider, settings
from llm.providers import LLMProviderFactory
from llm.prompts import PromptTemplates

# Load environment variables
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass


async def test_workflow_json():
    """Test the complete workflow JSON generation."""
    
    print("üîç Testing Workflow JSON Generation")
    print("=" * 35)
    
    # Check API key
    api_key = os.getenv("OPENROUTER_API_KEY")
    if not api_key:
        print("‚ùå No OPENROUTER_API_KEY found")
        return
    
    print(f"‚úÖ API key found: {api_key[:15]}...")
    
    # Configure for workflow (more conservative settings)
    config = LLMConfig(
        provider=LLMProvider.OPENROUTER,
        model="deepseek/deepseek-chat-v3.1:free",
        temperature=0.1,  # Very low for consistency
        max_tokens=4000,  # Higher limit for complex responses
        api_key=api_key,
        region=None
    )
    
    print(f"üìã Config: {config.provider.value} - {config.model}")
    
    try:
        # Create provider
        provider = LLMProviderFactory.create(config, "workflow_test_agent")
        await provider.initialize()
        print("‚úÖ Provider initialized")
        
        # Test the actual workflow prompt
        print("\nüß™ Testing Full Workflow Prompt")
        
        # Use the actual prompt from the system
        file_path = "test_file.csv"  # Mock file path
        workflow_prompt = PromptTemplates.FILE_READER_LANGGRAPH_MCP_WORKFLOW.format(file_path=file_path)
        
        # Add system message for more control
        system_message = """You are a precise file processing agent. You must:
1. Follow the sequential steps exactly
2. Always return valid JSON
3. If any step fails, return {"success": false, "error": "description"}
4. Never return plain text responses
5. Always end with ONLY the JSON object"""
        
        print("üì§ Sending workflow prompt...")
        
        try:
            response = await provider.generate(
                workflow_prompt,
                system_message=system_message,
                temperature=0.1,
                max_tokens=4000
            )
            print(f"üì• Response length: {len(response)} characters")
            print(f"üì• First 200 chars: {response[:200]}...")
            print(f"üì• Last 200 chars: ...{response[-200:]}")
            
            # Try to parse as JSON
            import json
            try:
                parsed = json.loads(response)
                print(f"‚úÖ Valid JSON Response!")
                print(f"üîë Keys: {list(parsed.keys()) if isinstance(parsed, dict) else 'Not a dict'}")
                
                # Check expected structure
                if isinstance(parsed, dict):
                    if "success" in parsed:
                        print(f"‚úÖ Has 'success' field: {parsed['success']}")
                    if "error" in parsed:
                        print(f"‚ö†Ô∏è Has 'error' field: {parsed['error']}")
                    if "file_data" in parsed:
                        print(f"‚úÖ Has 'file_data' field")
                    if "metadata" in parsed:
                        print(f"‚úÖ Has 'metadata' field")
                        
            except json.JSONDecodeError as e:
                print(f"‚ùå JSON Parse Error: {e}")
                print(f"üìù Full response: {repr(response)}")
                
                # Try to find JSON in response
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    try:
                        extracted_json = json_match.group()
                        parsed = json.loads(extracted_json)
                        print(f"‚úÖ Extracted valid JSON: {list(parsed.keys())}")
                    except:
                        print(f"‚ùå Extracted JSON also invalid")
                else:
                    print(f"‚ùå No JSON pattern found in response")
        
        except Exception as e:
            print(f"‚ùå Generation Error: {e}")
            print(f"üìù Error type: {type(e).__name__}")
            
            # Check if it's a server error
            if "server had an error" in str(e).lower():
                print("üîÑ Server error detected - this is the root cause!")
                print("üí° Suggestions:")
                print("   1. Reduce prompt complexity")
                print("   2. Add retry logic")
                print("   3. Use simpler model")
                print("   4. Check rate limits")
            
    except Exception as e:
        print(f"‚ùå Provider Error: {e}")
        import traceback
        traceback.print_exc()


async def test_simple_prompt():
    """Test with a much simpler prompt."""
    print("\nüß™ Testing Simplified Prompt")
    
    api_key = os.getenv("OPENROUTER_API_KEY")
    config = LLMConfig(
        provider=LLMProvider.OPENROUTER,
        model="deepseek/deepseek-chat-v3.1:free",
        temperature=0.1,
        max_tokens=1000,  # Much smaller
        api_key=api_key,
        region=None
    )
    
    provider = LLMProviderFactory.create(config, "simple_test_agent")
    await provider.initialize()
    
    simple_prompt = """Process this sample CSV data and return JSON:
name,age,city
John,30,NYC
Jane,25,LA

Return this exact format:
{
    "success": true,
    "file_data": [
        {"name": "John", "age": 30, "city": "NYC"},
        {"name": "Jane", "age": 25, "city": "LA"}
    ],
    "metadata": {
        "total_records": 2,
        "file_type": "csv"
    }
}"""
    
    try:
        response = await provider.generate(simple_prompt)
        print(f"üì• Simple response: {response}")
        
        import json
        parsed = json.loads(response)
        print(f"‚úÖ Simple JSON valid: {list(parsed.keys())}")
        
    except Exception as e:
        print(f"‚ùå Simple test failed: {e}")


if __name__ == "__main__":
    asyncio.run(test_workflow_json())
    asyncio.run(test_simple_prompt())
